stuff to do
- condense the file: check
- add function to main program: check
- run twice, document and summary: check
- get the key words
- compare the vectors of the keywords

- extract target & context weights from summary word2vec check!


What we learned
- our summary is not long enough to create training examples
- can't have all text in one line, but also need enough words

- instead of training a serparte word2vec model with the summary text, 
we could use the weights of the document word2vec and try to evaluate the summary
using n-skip grams

Bayes Theorem
    - probability is the math of proportions
    - look at the cases where the evidence is true,
    then look at the cases where the hypothesis is also true
    - a grade is just a probability of being correct, and a probability is just a proportion
    - find out what the evidence, and hypothesis is for the summary problem

Research areas
    - does the text change a word's embedding?
    will the word "Sun" be linked to the same words in two different text
