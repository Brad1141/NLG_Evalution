{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load('multi_news', split='train', with_info=True)\n",
    "\n",
    "#use index to get specific document\n",
    "index = 20\n",
    "count = 0\n",
    "with tf.Graph().as_default():\n",
    "    numpy_imgs = next(iter(ds))\n",
    "    # numpy_imgs = tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "document = []\n",
    "summary = []\n",
    "for x in numpy_imgs:\n",
    "    count += 1\n",
    "    if count == index:\n",
    "        # tf.print(x[\"document\"])\n",
    "        # print(\"\\n\")\n",
    "        # print(\"\\n\")\n",
    "        # print(\"SUMMARY\")\n",
    "        # tf.print(x[\"summary\"])\n",
    "\n",
    "        document = x[\"document\"]\n",
    "        summary = x[\"summary\"]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = bytes(document.numpy())\n",
    "document = [document.decode(\"utf-8\")]\n",
    "\n",
    "summary = bytes(summary.numpy())\n",
    "summary = [summary.decode(\"utf-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vocab\n",
    "d_tokens = document[0].lower().split()\n",
    "s_tokens = summary[0].lower().split()\n",
    "tokens = d_tokens + s_tokens\n",
    "vocab, index = {}, 1\n",
    "vocab[\"<pad>\"] = 0\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index = index + 1\n",
    "\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "example = [vocab[word] for word in s_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.27107972 -0.01055073 -0.05728397  0.06853679 -0.08438271  0.22396211\n",
      "  -0.00247001 -0.09797598 -0.06092518  0.01678422  0.0183306  -0.02683547\n",
      "   0.01987647  0.02205245  0.0380337   0.02345292 -0.0535214  -0.02916854\n",
      "  -0.13816142  0.255649    0.00548296  0.08994407  0.09702856 -0.01617393\n",
      "   0.15273312  0.03449007  0.05599031  0.01964826 -0.01901525  0.11601479\n",
      "   0.06575833 -0.03560898 -0.02412845 -0.00716866 -0.08950593 -0.01021391\n",
      "   0.07431487 -0.10462939 -0.03951982  0.00272066 -0.01468687 -0.01350653\n",
      "  -0.04825642  0.03088917 -0.0448269  -0.01743765  0.1034883   0.04149228\n",
      "  -0.03979184  0.03878277  0.15273733 -0.09228262 -0.01723959  0.01830614\n",
      "  -0.02075483  0.0800882  -0.08071491 -0.15573218  0.13893387  0.06140287\n",
      "  -0.05639812 -0.05526257 -0.02765993 -0.175832    0.01034007 -0.19550695\n",
      "   0.06209265 -0.03193678  0.08837268 -0.05629309  0.09268684  0.05395978\n",
      "  -0.01900527 -0.17626993 -0.03103094 -0.12204378 -0.00157093 -0.07853678\n",
      "  -0.05892268  0.11311311  0.00749602  0.17720635  0.00705703 -0.02477936\n",
      "   0.01754808 -0.10708189  0.01354044 -0.02276768  0.18480518  0.23878463\n",
      "   0.02690842  0.17486377  0.15445334  0.04336881  0.10030321  0.03473791\n",
      "   0.13695279 -0.01135186  0.10339782 -0.04216714  0.15414187  0.00816383\n",
      "  -0.09016436  0.05382214 -0.00468258 -0.03378959 -0.06590898  0.03239544\n",
      "  -0.05416648  0.00577813  0.00359992 -0.1197789   0.03960301 -0.09633037\n",
      "  -0.03632317  0.04465347  0.08645485  0.03435422 -0.01637179 -0.02096656\n",
      "   0.10808575 -0.09299634  0.06655572 -0.04052928 -0.08087688 -0.1576648\n",
      "   0.03123586  0.02210312]\n",
      " [ 0.24816465 -0.06619297 -0.07488655 -0.0434025  -0.08399508  0.11924921\n",
      "  -0.07240637 -0.06224827 -0.0619347  -0.10450241  0.09173684 -0.0157325\n",
      "  -0.00665783 -0.03739307  0.04591279 -0.10748252 -0.03960943 -0.01221685\n",
      "  -0.15646328  0.28827062 -0.05034578  0.06034692 -0.06292915 -0.12956862\n",
      "   0.06275239  0.05485379  0.10006846 -0.05434096 -0.07562272  0.02795899\n",
      "   0.02877025 -0.03873997  0.08480944  0.10531907 -0.07284793 -0.00243958\n",
      "  -0.06184474 -0.11018215 -0.05470686  0.02247239 -0.02980752 -0.02966652\n",
      "  -0.02023029 -0.05196458  0.08784661 -0.01775759 -0.03682138 -0.04811873\n",
      "  -0.09006107  0.10117017  0.10831158 -0.01668864  0.07171492  0.07705168\n",
      "   0.02387583 -0.0261501  -0.04829222  0.04328855  0.02772755 -0.05743307\n",
      "  -0.01383987 -0.0569593  -0.06737578 -0.09332056  0.02847968 -0.10889357\n",
      "  -0.03210738 -0.05884559  0.05467662 -0.08974586 -0.07166561  0.03896518\n",
      "   0.10429208 -0.06946753  0.00462049 -0.1334935  -0.15643585  0.01501083\n",
      "  -0.10304678  0.13439935 -0.00136194  0.04372552  0.00499131  0.02251436\n",
      "  -0.07825701  0.01211459 -0.10891134 -0.09566517  0.19746777  0.20468362\n",
      "  -0.01937577  0.08558907 -0.02050492 -0.07046179  0.07654233  0.04140148\n",
      "   0.221628   -0.12482118  0.11911826 -0.1242468   0.12674199 -0.00684475\n",
      "  -0.0930409   0.08155327  0.00574383 -0.0432922  -0.07563888  0.01789355\n",
      "  -0.01771837 -0.08992799  0.00103209 -0.08330975  0.14422818  0.09882832\n",
      "  -0.02616246  0.02361001  0.13217404 -0.02489127 -0.02765685 -0.07560521\n",
      "   0.01649955 -0.09440068 -0.06909272 -0.08880886 -0.06736004 -0.13825142\n",
      "   0.17052694  0.12467341]], shape=(2, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
    "embeddings = embed([\"cat is on the mat\", \"dog is in the fog\"])\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: C:\\Users\\Brad\\AppData\\Local\\Temp\\tfhub_modules\\602d30248ff7929470db09f7385fc895e9ceb4c0\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-7949aff29705>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#lean word2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpreprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mbert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://tfhub.dev/google/experts/bert/wiki_books/2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[0;32m    105\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    862\u001b[0m   \"\"\"\n\u001b[0;32m    863\u001b[0m   \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementReadApi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LOAD_V2_LABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"root\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m   \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m   saved_model_proto, debug_info = (\n\u001b[1;32m--> 878\u001b[1;33m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m   \"\"\"\n\u001b[1;32m---> 60\u001b[1;33m   \u001b[0msaved_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m   debug_info_path = os.path.join(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    116\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     raise IOError(\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: C:\\Users\\Brad\\AppData\\Local\\Temp\\tfhub_modules\\602d30248ff7929470db09f7385fc895e9ceb4c0\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# stuff to do\n",
    "# get vocabulary\n",
    "# display output in a visual way\n",
    "#lean word2vec\n",
    "\n",
    "preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bert = hub.load('https://tfhub.dev/google/experts/bert/wiki_books/2')\n",
    "\n",
    "sentences = [\n",
    "  \"Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.\",\n",
    "  \"The album went straight to number one on the Norwegian album chart, and sold to double platinum.\",\n",
    "  \"Among the singles released from the album were the songs \\\"Be My Lover\\\" and \\\"Hard To Stay Awake\\\".\",\n",
    "  \"Riccardo Zegna is an Italian jazz musician.\",\n",
    "  \"Rajko Maksimović is a composer, writer, and music pedagogue.\",\n",
    "  \"One of the most significant Serbian composers of our time, Maksimović has been and remains active in creating works for different ensembles.\",\n",
    "  \"Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum\",\n",
    "  \"A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.\",\n",
    "  \"A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth.\",\n",
    "]\n",
    "\n",
    "wordArray = []\n",
    "\n",
    "for i in sentences:\n",
    "  words = i.split()\n",
    "  for w in words:\n",
    "    w = w.replace(\",\", \"\")\n",
    "    w = w.replace(\".\", \"\")\n",
    "    wordArray.append(w)\n",
    "\n",
    "print(\"word array\")\n",
    "print(wordArray)\n",
    "\n",
    "bert_inputs = preprocess(sentences)\n",
    "bert_outputs = bert(bert_inputs)\n",
    "pooled_output = bert_outputs['pooled_output']\n",
    "sequence_output = bert_outputs['sequence_output']\n",
    "\n",
    "print('\\nPooled output:')\n",
    "print(pooled_output)\n",
    "print('\\nSequence output:')\n",
    "print(sequence_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-f788641eb1e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Semantic Textual Similarity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mplot_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pooled_output\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_similarity(features, labels):\n",
    "  \"\"\"Plot a similarity matrix of the embeddings.\"\"\"\n",
    "  cos_sim = pairwise.cosine_similarity(features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  cbar_kws=dict(use_gridspec=False, location=\"left\")\n",
    "  g = sns.heatmap(\n",
    "      cos_sim, xticklabels=labels, yticklabels=labels,\n",
    "      vmin=0, vmax=1, cmap=\"Blues\", cbar_kws=cbar_kws)\n",
    "  g.tick_params(labelright=True, labelleft=False)\n",
    "  g.set_yticklabels(labels, rotation=0)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "plot_similarity(bert_outputs[\"pooled_output\"], wordArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "num_ns = 4\n",
    "\n",
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordVectors(textIdx, useBoard):\n",
    "    file_path = textIdx\n",
    "    text_ds = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "    \n",
    "    # Define the vocabulary size and number of words in a sequence.\n",
    "    vocab_size = 4096\n",
    "    sequence_length = 10\n",
    "\n",
    "    # Use the TextVectorization layer to normalize, split, and map strings to\n",
    "    # integers. Set output_sequence_length length to pad all samples to same length.\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length)\n",
    "\n",
    "    vectorize_layer.adapt(text_ds.batch(1024))\n",
    "\n",
    "    inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "    sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "    \n",
    "    targets, contexts, labels = generate_training_data(\n",
    "        sequences=sequences,\n",
    "        window_size=2,\n",
    "        num_ns=4,\n",
    "        vocab_size=vocab_size,\n",
    "        seed=SEED)\n",
    "\n",
    "    targets = np.array(targets)\n",
    "    contexts = np.array(contexts)[:,:,0]\n",
    "    print(\"context\")\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # BATCH_SIZE = 1024\n",
    "    # BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 10\n",
    "    BUFFER_SIZE = 15\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    embedding_dim = 128\n",
    "    word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "    word2vec.compile(optimizer='adam',\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    if useBoard:\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "        word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n",
    "    else:\n",
    "        word2vec.fit(dataset, epochs=20)\n",
    "\n",
    "    weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    return weights, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1004.38it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-b37c4be29073>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# doc_weights, doc_vocab = createWordVectors(\"document.txt\", False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msum_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateWordVectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"summary.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"document weights\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-64681b06ce6e>\u001b[0m in \u001b[0;36mcreateWordVectors\u001b[1;34m(textIdx, useBoard)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mcontexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"context\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "# doc_weights, doc_vocab = createWordVectors(\"document.txt\", False)\n",
    "sum_weights, sum_vocab = createWordVectors(\"summary.txt\", False)\n",
    "\n",
    "print(\"document weights\")\n",
    "print(doc_weights)\n",
    "\n",
    "print(\"summary weights\")\n",
    "print(sum_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c931ba401747e1100110d99c7b2e1195adf3961a7e00160e720e39c4d164b397"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
