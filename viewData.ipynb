{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "import os\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load('multi_news', split='train', with_info=True)\n",
    "\n",
    "#use index to get specific document\n",
    "index = 20\n",
    "count = 0\n",
    "with tf.Graph().as_default():\n",
    "    numpy_imgs = next(iter(ds))\n",
    "    # numpy_imgs = tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "document = []\n",
    "summary = []\n",
    "for x in numpy_imgs:\n",
    "    count += 1\n",
    "    if count == index:\n",
    "        # tf.print(x[\"document\"])\n",
    "        # print(\"\\n\")\n",
    "        # print(\"\\n\")\n",
    "        # print(\"SUMMARY\")\n",
    "        # tf.print(x[\"summary\"])\n",
    "\n",
    "        document = x[\"document\"]\n",
    "        summary = x[\"summary\"]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = bytes(document.numpy())\n",
    "document = [document.decode(\"utf-8\")]\n",
    "\n",
    "summary = bytes(summary.numpy())\n",
    "summary = [summary.decode(\"utf-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vocab\n",
    "d_tokens = document[0].lower().split()\n",
    "s_tokens = summary[0].lower().split()\n",
    "tokens = d_tokens + s_tokens\n",
    "vocab, index = {}, 1\n",
    "vocab[\"<pad>\"] = 0\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index = index + 1\n",
    "\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "example = [vocab[word] for word in s_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
    "embeddings = embed([\"cat is on the mat\", \"dog is in the fog\"])\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff to do\n",
    "# get vocabulary\n",
    "# display output in a visual way\n",
    "#lean word2vec\n",
    "\n",
    "preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bert = hub.load('https://tfhub.dev/google/experts/bert/wiki_books/2')\n",
    "\n",
    "sentences = [\n",
    "  \"Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.\",\n",
    "  \"The album went straight to number one on the Norwegian album chart, and sold to double platinum.\",\n",
    "  \"Among the singles released from the album were the songs \\\"Be My Lover\\\" and \\\"Hard To Stay Awake\\\".\",\n",
    "  \"Riccardo Zegna is an Italian jazz musician.\",\n",
    "  \"Rajko Maksimović is a composer, writer, and music pedagogue.\",\n",
    "  \"One of the most significant Serbian composers of our time, Maksimović has been and remains active in creating works for different ensembles.\",\n",
    "  \"Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum\",\n",
    "  \"A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.\",\n",
    "  \"A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth.\",\n",
    "]\n",
    "\n",
    "wordArray = []\n",
    "\n",
    "for i in sentences:\n",
    "  words = i.split()\n",
    "  for w in words:\n",
    "    w = w.replace(\",\", \"\")\n",
    "    w = w.replace(\".\", \"\")\n",
    "    wordArray.append(w)\n",
    "\n",
    "print(\"word array\")\n",
    "print(wordArray)\n",
    "\n",
    "bert_inputs = preprocess(sentences)\n",
    "bert_outputs = bert(bert_inputs)\n",
    "pooled_output = bert_outputs['pooled_output']\n",
    "sequence_output = bert_outputs['sequence_output']\n",
    "\n",
    "print('\\nPooled output:')\n",
    "print(pooled_output)\n",
    "print('\\nSequence output:')\n",
    "print(sequence_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f788641eb1e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Semantic Textual Similarity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mplot_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pooled_output\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_similarity(features, labels):\n",
    "  \"\"\"Plot a similarity matrix of the embeddings.\"\"\"\n",
    "  cos_sim = pairwise.cosine_similarity(features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  cbar_kws=dict(use_gridspec=False, location=\"left\")\n",
    "  g = sns.heatmap(\n",
    "      cos_sim, xticklabels=labels, yticklabels=labels,\n",
    "      vmin=0, vmax=1, cmap=\"Blues\", cbar_kws=cbar_kws)\n",
    "  g.tick_params(labelright=True, labelleft=False)\n",
    "  g.set_yticklabels(labels, rotation=0)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "plot_similarity(bert_outputs[\"pooled_output\"], wordArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "num_ns = 4\n",
    "\n",
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordVectors(textIdx, useBoard):\n",
    "    file_path = textIdx\n",
    "    text_ds = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "    \n",
    "    # Define the vocabulary size and number of words in a sequence.\n",
    "    vocab_size = 4096\n",
    "    sequence_length = 10\n",
    "\n",
    "    # Use the TextVectorization layer to normalize, split, and map strings to\n",
    "    # integers. Set output_sequence_length length to pad all samples to same length.\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length)\n",
    "\n",
    "    vectorize_layer.adapt(text_ds.batch(1024))\n",
    "\n",
    "    inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "    sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "    \n",
    "    targets, contexts, labels = generate_training_data(\n",
    "        sequences=sequences,\n",
    "        window_size=2,\n",
    "        num_ns=4,\n",
    "        vocab_size=vocab_size,\n",
    "        seed=SEED)\n",
    "\n",
    "    targets = np.array(targets)\n",
    "    print(\"array shape\")\n",
    "    print(targets.shape)\n",
    "    contexts = np.array(contexts)[:,:,0]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # BATCH_SIZE = 1024\n",
    "    # BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 10\n",
    "    BUFFER_SIZE = 15\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    embedding_dim = 128\n",
    "    word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "    word2vec.compile(optimizer='adam',\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    if useBoard:\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "        word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n",
    "    else:\n",
    "        word2vec.fit(dataset, epochs=20)\n",
    "\n",
    "    weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    return weights, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3423/3423 [00:01<00:00, 1943.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array shape\n",
      "(11244,)\n",
      "Epoch 1/20\n",
      "1124/1124 [==============================] - 16s 13ms/step - loss: 1.6072 - accuracy: 0.2181\n",
      "Epoch 2/20\n",
      "1124/1124 [==============================] - 11s 10ms/step - loss: 1.4555 - accuracy: 0.8331\n",
      "Epoch 3/20\n",
      "1124/1124 [==============================] - 10s 9ms/step - loss: 1.0666 - accuracy: 0.8765\n",
      "Epoch 4/20\n",
      "1124/1124 [==============================] - 13s 12ms/step - loss: 0.6638 - accuracy: 0.9153\n",
      "Epoch 5/20\n",
      "1124/1124 [==============================] - 15s 13ms/step - loss: 0.3901 - accuracy: 0.9455\n",
      "Epoch 6/20\n",
      "1124/1124 [==============================] - 17s 15ms/step - loss: 0.2328 - accuracy: 0.9713\n",
      "Epoch 7/20\n",
      "1124/1124 [==============================] - 17s 16ms/step - loss: 0.1477 - accuracy: 0.9844\n",
      "Epoch 8/20\n",
      "1124/1124 [==============================] - 18s 16ms/step - loss: 0.1012 - accuracy: 0.9909\n",
      "Epoch 9/20\n",
      "1124/1124 [==============================] - 19s 17ms/step - loss: 0.0745 - accuracy: 0.99300s - loss: 0.0749 - accuracy\n",
      "Epoch 10/20\n",
      "1124/1124 [==============================] - 19s 17ms/step - loss: 0.0586 - accuracy: 0.99380s - loss: 0.0589 - accuracy\n",
      "Epoch 11/20\n",
      "1124/1124 [==============================] - 17s 15ms/step - loss: 0.0489 - accuracy: 0.9938\n",
      "Epoch 12/20\n",
      "1124/1124 [==============================] - 17s 15ms/step - loss: 0.0427 - accuracy: 0.9945\n",
      "Epoch 13/20\n",
      "1124/1124 [==============================] - 16s 14ms/step - loss: 0.0388 - accuracy: 0.9946\n",
      "Epoch 14/20\n",
      "1124/1124 [==============================] - 15s 13ms/step - loss: 0.0363 - accuracy: 0.9945\n",
      "Epoch 15/20\n",
      "1124/1124 [==============================] - 13s 12ms/step - loss: 0.0347 - accuracy: 0.9946\n",
      "Epoch 16/20\n",
      "1124/1124 [==============================] - 13s 12ms/step - loss: 0.0336 - accuracy: 0.99460s - loss: 0.0339 - accu\n",
      "Epoch 17/20\n",
      "1124/1124 [==============================] - 13s 11ms/step - loss: 0.0328 - accuracy: 0.9947\n",
      "Epoch 18/20\n",
      "1124/1124 [==============================] - 13s 11ms/step - loss: 0.0323 - accuracy: 0.9945\n",
      "Epoch 19/20\n",
      "1124/1124 [==============================] - 12s 11ms/step - loss: 0.0320 - accuracy: 0.99480s - loss: 0.0322 - accuracy: \n",
      "Epoch 20/20\n",
      "1124/1124 [==============================] - 12s 11ms/step - loss: 0.0317 - accuracy: 0.9945\n"
     ]
    }
   ],
   "source": [
    "# doc_weights, doc_vocab = createWordVectors(\"document.txt\", False)\n",
    "sum_weights, sum_vocab = createWordVectors(\"christmas_carol.txt\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(sum_vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = sum_weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "log_dir='logs'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Save Labels separately on a line-by-line manner.\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "  for subwords in encoder.subwords:\n",
    "    f.write(\"{}\\n\".format(subwords))\n",
    "  # Fill in the rest of the labels with \"unknown\".\n",
    "  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
    "    f.write(\"unknown #{}\\n\".format(unknown))\n",
    "\n",
    "\n",
    "# Save the weights we want to analyze as a variable. Note that the first\n",
    "# value represents any unknown word, which is not in the metadata, here\n",
    "# we will remove this value.\n",
    "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
    "# Create a checkpoint from embedding, the filename and key are the\n",
    "# name of the tensor.\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_weights, sum_vocab = createWordVectors(\"shortTest.txt\", False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c931ba401747e1100110d99c7b2e1195adf3961a7e00160e720e39c4d164b397"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
